{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from keras import backend\n",
    "from keras.layers import Conv1D, Dense, Input, Lambda, LSTM, GRU, RNN, CuDNNGRU, CuDNNLSTM, Dropout, Bidirectional\n",
    "from keras.layers.merge import concatenate\n",
    "from keras.layers.embeddings import Embedding\n",
    "from keras.models import Model\n",
    "from keras.utils import to_categorical, plot_model\n",
    "from keras.preprocessing.text import Tokenizer\n",
    "from keras.preprocessing import sequence\n",
    "from keras.regularizers import l2\n",
    "from keras.callbacks import EarlyStopping, ModelCheckpoint\n",
    "from keras.optimizers import Adam\n",
    "from string import punctuation\n",
    "from nltk.corpus import stopwords\n",
    "from matplotlib import pyplot as plt\n",
    "from sklearn.model_selection import train_test_split\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import gensim.downloader as api"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "vectors = api.load(\"glove-twitter-100\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def process_data(blogs, lower=True, remove_punc=True, split=True, remove_stop=False):\n",
    "    # lowercase all text\n",
    "    if lower:\n",
    "        blogs = [blog.lower() for blog in blogs]\n",
    "    # remove punctuation\n",
    "    if remove_punc:\n",
    "        blogs = [blog.translate(str.maketrans('', '', punctuation)).strip() for blog in blogs]\n",
    "    # split into individual tokens\n",
    "    if split:\n",
    "        blogs = [blog.split() for blog in blogs]\n",
    "    # remove stopwords\n",
    "    if remove_stop:\n",
    "        stop = set(stopwords.words('english'))\n",
    "        final_blogs = []\n",
    "        for blog in blogs:\n",
    "            b = []\n",
    "            for token in blog.split():\n",
    "                if token not in stop:\n",
    "                    b.append(token)\n",
    "            final_blogs.append(' '.join(b))\n",
    "        blogs = final_blogs\n",
    "    return blogs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "blog_data = pd.read_csv(\"blog-gender-dataset_csv.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "blog_data.Blog = process_data(blog_data.Blog.astype(str), split=False, remove_punc=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def process_gender(gender):\n",
    "    encoded_genders = []\n",
    "    for g in gender:\n",
    "        if g in ('M', 'm', 'male', 'Male'):\n",
    "            g = 1\n",
    "        else:\n",
    "            g = 0\n",
    "        encoded_genders.append(g)\n",
    "    return encoded_genders"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "blog_data.Gender = process_gender(blog_data.Gender.astype(str))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "embeddings = np.zeros((vectors.syn0.shape[0] + 1, vectors.syn0.shape[1]), dtype = \"float32\")\n",
    "embeddings[:vectors.syn0.shape[0]] = vectors.syn0\n",
    "\n",
    "MAX_TOKENS = vectors.syn0.shape[0]\n",
    "embedding_dim = vectors.syn0.shape[1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "input_docs = []\n",
    "input_lc = []\n",
    "input_rc = []\n",
    "max_len_contexts=199\n",
    "for blog in blog_data.Blog:\n",
    "    tokens = [vectors.vocab[token].index if token in vectors.vocab else MAX_TOKENS for token in blog.split()]\n",
    "    doc = np.array(tokens)\n",
    "    left_context = np.array([MAX_TOKENS] + tokens[:max_len_contexts])\n",
    "    right_context = np.array(tokens[1:max_len_contexts + 1] + [MAX_TOKENS])\n",
    "    input_docs.append(doc)\n",
    "    input_lc.append(left_context)\n",
    "    input_rc.append(right_context)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "max_len=500\n",
    "input_docs = sequence.pad_sequences(input_docs, maxlen=max_len, padding='post', truncating='post', value=0)\n",
    "input_lc = sequence.pad_sequences(input_lc, maxlen=max_len, padding='post', truncating='post', value=0)\n",
    "input_rc = sequence.pad_sequences(input_rc, maxlen=max_len, padding='post', truncating='post', value=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "splits = train_test_split(input_docs, input_lc, input_rc, blog_data.Gender, test_size=0.1, shuffle=True, stratify=blog_data.Gender)\n",
    "input_docs_train, input_docs_test = splits[0], splits[1]\n",
    "input_lc_train, input_lc_test = splits[2], splits[3]\n",
    "input_rc_train, input_rc_test = splits[4], splits[5]\n",
    "genders_train, genders_test = splits[6], splits[7]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def RCNN(embeddings, MAX_TOKENS, embedding_dim, hidden_1=100, hidden_2=100):\n",
    "    document = Input(shape = (None, ), dtype = \"int32\")\n",
    "    left_context = Input(shape = (None, ), dtype = \"int32\")\n",
    "    right_context = Input(shape = (None, ), dtype = \"int32\")\n",
    "    \n",
    "    embedder = Embedding(MAX_TOKENS + 1, embedding_dim, weights = [embeddings], trainable = True)\n",
    "    doc_embedding = embedder(document)\n",
    "    l_embedding = embedder(left_context)\n",
    "    r_embedding = embedder(right_context)\n",
    "    \n",
    "    doc_embedding = Dropout(0.1)(doc_embedding)\n",
    "    l_embedding = Dropout(0.1)(l_embedding)\n",
    "    r_embedding = Dropout(0.1)(r_embedding)\n",
    "    \n",
    "    forward = CuDNNLSTM(hidden_1, kernel_initializer='glorot_normal', return_sequences = True, activity_regularizer=l2(0.007))(l_embedding) # See equation (1).\n",
    "    backward = CuDNNLSTM(hidden_1, kernel_initializer='glorot_normal', return_sequences = True, go_backwards = True, activity_regularizer=l2(0.007))(r_embedding) # See equation (2).\n",
    "\n",
    "    backward = Lambda(lambda x: backend.reverse(x, axes = 1))(backward)\n",
    "    together = concatenate([forward, doc_embedding, backward], axis = 2)\n",
    "        \n",
    "    rcnn = Conv1D(hidden_2, kernel_size=1, padding='same', activation = \"tanh\", activity_regularizer=l2(0.007))(together) # See equation (4).\n",
    "    \n",
    "    pool_rcnn = Lambda(lambda x: backend.max(x, axis = 1), output_shape = (hidden_2, ))(rcnn) # See equation (5).\n",
    "    \n",
    "    dense = Dense(64, activity_regularizer=l2())(pool_rcnn)\n",
    "    \n",
    "    dense = Dropout(0.2)(dense)\n",
    "    \n",
    "    dense = Dense(32)(dense)\n",
    "    \n",
    "    output = Dense(1, input_dim = hidden_2, activation = \"sigmoid\")(dense) # See equations (6) and (7).\n",
    "\n",
    "    model = Model(inputs = [document, left_context, right_context], outputs = output)\n",
    "    model.compile(optimizer = \"adam\", loss = \"binary_crossentropy\", metrics = [\"accuracy\"])\n",
    "    \n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = RCNN(embeddings, MAX_TOKENS, embedding_dim)\n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "history = model.fit([input_docs_train, input_lc_train, input_rc_train], genders_train, \n",
    "                    epochs=200, verbose=1, batch_size=64, validation_split=0.1, shuffle=True])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.plot(history.history['acc'])\n",
    "plt.plot(history.history['val_acc'])\n",
    "plt.title('model accuracy')\n",
    "plt.ylabel('accuracy')\n",
    "plt.xlabel('epoch')\n",
    "plt.legend(['train', 'val'], loc='upper left')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.plot(history.history['loss'][10:])\n",
    "plt.plot(history.history['val_loss'][10:])\n",
    "plt.title('model loss')\n",
    "plt.ylabel('loss')\n",
    "plt.xlabel('epoch')\n",
    "plt.legend(['train', 'val'], loc='upper left')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.evaluate([input_docs_test, input_lc_test, input_rc_test], genders_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_model(model, to_file='model.png')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
